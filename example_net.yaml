# Section that defines the dataset files.
dataset:
  # Training file, used for training the network.
  training:
    paths:
      - dataset/training.tfrecord
    # The batch size to use when training the network
    batch_size: 20

    # Additional configuration that will be merged into the view, example, orientation, label and projection config
    config:
      orientation:
        augmentations:
          # Adjustments to the height of the mesh above the observation plane.
          height: { mean: 0, stddev: 0.05 }
          # Adjustments to the orientation of the observation plane
          # We choose a random axis and rotate by a random angle from this distribution
          rotation: { mean: 0, stddev: 0.0872665 }

  # Validation file, used to inspect progress of the network as it is training.
  # It is also used as the source for progress images.
  validation:
    paths:
      - dataset/validation.tfrecord
    batch_size: 50

  # Test dataset, used for performing final evaluations on the performance of the network.
  testing:
    paths:
      - dataset/testing.tfrecord
    batch_size: 200

# What camera configuration we have (Monoscopic, Stereoscopic)
view:
  type: Monoscopic
  config: {}

# What kind of examples we are using for input data (Image)
example:
  type: Image
  config: {}

# What kind of mesh orientation we are using (Ground, Spotlight)
orientation:
  type: Ground
  config: {}

# The type of output we are training the network for (Classification, Seeker)
label:
  type: Classification
  config:
    # The classes we are using for classification
    classes:
      - name: ball
        colours:
          - [255, 0, 0]
      - name: goal
        colours:
          - [255, 255, 0]
      - name: line
        colours:
          - [255, 255, 255]
      - name: field
        colours:
          - [0, 255, 0]
      - name: environment
        colours:
          - [0, 0, 0]

# What kind of graph we are projecting (VisualMesh)
projection:
  type: VisualMesh
  # The configuration for the various components of the flavour
  config:
    # The type of mesh we are using
    mesh:
      # The type of Visual Mesh we are generating
      model: RING6
      # How many distinct meshes to cache before dropping old ones
      cached_meshes: 100
      # The maximum distance the Visual Mesh will be projected for. This should be slightly further than the most distant
      # object that you wish to detect to account for noises in the projection.
      max_distance: 20
      # Edge type, if when we travel beyond the mesh (off the screen or off a mesh edge) what should the graph behaviour be
      # HARD: Have a graph node that connects to itself with a -1 input value
      # SAME: When a graph node would travel off an edge, connect back to itself
      edge_type: SAME

    # The geometry of the object we are detecting in the Visual Mesh.
    geometry:
      # The shape to project, either CIRCLE or SPHERE.
      shape: SPHERE
      # The radius of the object to be detected.
      radius: 0.0949996
      # How many intersections with the target object we should have.
      intersections: 6
      # How many intersections the mesh can vary by before it will generate a new mesh
      intersection_tolerance: 0.5

# Configuration for the convolutional neural network.
network:
  # The network structure defined as a graph of ops where each element will define its inputs.
  # There are two predefined inputs to the graph which are `X` and `G` which represent the input values to the network
  # and the neighbourhood graph respectively.
  # There should exist a node named `output` that defines the final output of the network.
  # The network will start at this element and work back from there to find which components are needed and in what order.
  # For each of the elements they will define an operation they are doing, and optionally provide options for constructing
  # The final output dimensionality (which will be the size of the dataset output) can be accessed with `$output_dims`
  # This variable name if placed anywhere in the structure options will be replaced with the integer number of outputs
  # the dataset will produce
  structure:
    l1: { op: GraphConvolution, inputs: [X, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l2: { op: GraphConvolution, inputs: [l1, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l3: { op: GraphConvolution, inputs: [l2, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l4: { op: GraphConvolution, inputs: [l3, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l5: { op: GraphConvolution, inputs: [l4, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l6: { op: GraphConvolution, inputs: [l5, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l7: { op: GraphConvolution, inputs: [l6, G], options: { units: 16, activation: selu, kernel_initializer: lecun_normal } }
    l8: { op: GraphConvolution, inputs: [l7, G], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    l9: { op: GraphConvolution, inputs: [l8, G], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    l10: { op: GraphConvolution, inputs: [l9, G], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    l11: { op: GraphConvolution, inputs: [l10, G], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    l12: { op: GraphConvolution, inputs: [l11, G], options: { units: 8, activation: selu, kernel_initializer: lecun_normal } }
    output: { op: GraphConvolution, inputs: [l12, G], options: { units: $output_dims, activation: softmax, kernel_initializer: lecun_normal } }

# Testing
testing:
  # The number of points that will be used on curves
  n_points: 1000

# Settings to use when training the network
training:
  # Number of batches to consider an epoch, if None then it is the length of the input dataset
  batches_per_epoch: 1000
  # Number of epochs to execute
  epochs: 500

  # Optimiser settings
  optimiser:
    type: Ranger
    sync_period: 6
    slow_step_size: 0.5
  # optimiser:
  #   type: Adam
  # optimiser:
  #   type: SGD

  # Learning rate settings
  learning_rate:
    type: static
    value: 1e-3

  # learning_rate:
  #   type: one_cycle
  #   # Minimum learning rate to cycle up from
  #   min_learning_rate: 1e-5
  #   # Maximum learning rate at peak
  #   max_learning_rate: 1e-3
  #   # Learning rate to decay down to during the decay step
  #   decay_learning_rate: 1e-6
  #   # Number of epochs to do in the cycle
  #   cycle_epochs: 300
  #   # If hot_start is true, the training will restart it's training as if beginning at 0 without resetting the network
  #   # This means the one cycle will begin again treating the current global_step as the 0 point and going
  #   # cycle_batches + decay_batches more
  #   hot_start: False

  # Settings for the validation step of the network.
  validation:
    # How many batches to load for the validation step
    samples: 10
    # Whether to log metrics to tensorflow per batch or per epoch
    log_frequency: batch
    # How many images to show in tensorboard, they are taken as the first n images of the validation set.
    progress_images: 20
